\chapter{Conclusion}

To evaluate wheter the go plugin api is feasible for just in time compilation
one must consider the usability and robustness of the resulting implementation.
Furthermore it is crusial to take the resulting performance impact of the
solution into account, while not neglecting the complexity of the
implementation. The following section includes in depth examination of the
above criteria to conclude on the aforementioned consideration and introduces
other approaches to improve the performance of the runtime.

\section{Usability and Robustness}

The largest limiting usability factor is the requirement to have a local copy
of the go toolchain available. This makes user interaction in the way of
installing the go compiler necessary. 

The just in time compiler is engineered to be robust and not interfere with the
runtime when encoutering issues while generating code, invoking the tool-chain
or collecting meta data. Therefore the runtime does not halt execution upon
failing to call a just in time compiled function or if the just in time
compilation of a function fails. The runtime and just in time compiler
interaction allow for a seamless transition between compiled funciton and
interpreted function if necessary, such as after compiling a function or
bailing out of a compiled funciton. This is possible due to the inherently
concurrent nature of the just in time compilation pipeline. 

\section{Performance}

Based on performed benchmarks, their results and visual representation (see
\autoref{sec:benchmarks}), the performance impact of the JIT-Compiler is
substantial for most workloads, but especially for larger iterations, such as
in hot paths. The Go compiler optimises compiled plugins as if it were to
compile a Go application, such as reducing memory allocations\cite[Escape
Analysis]{go_wiki_optimization}, inlining
functions\cite[Inlining]{go_wiki_optimization} and not allocating empty
structures \cite[Interface Values]{go_wiki_optimization}. This allows the
interpreter to leverage the optimized performance the Go compiler applies to
binaries it compiles, thus enabling the observed large performance improvements
for string manipulation, arithmetic operations and the less substantial
improvements shown for the real world benchmarks.

However the overhead of walking the tree to generate go code from each node,
writing this code to a file, invoking the local go compiler via the
\texttt{os/exec} package, loading the go plugin and looking the compiled
function up, can not be neglected. Due to said influences Jit compilation
is inherently unsuitable for small workloads and fast executing inputs, as seen
on the workloads, inputs and iteration counts choosen for the benchmarks. 

\section{Implementation Complexity}

Due to the architecture of the runtime, the abstract syntax tree for a given
node is available while evaluating said node. The just-in-time compiler can
therefore use this tree to generate go code for every encountered node.
Furthermore the just-in-time compiler uses the existing runtime facilities for
performing meta tracing and asserting types. While the runtime architecture is
advantageous for the above, it made designing and implementing the just-in-time
compiler itself complex. There are several reasons for this, foremost the
package architecture, the just-in-time compiler has to depend on multiple
packages and use type assertions for custom types defined in subpackages, thus
it can not be extracted into a \texttt{jit} package. Furthermore the runtime
was designed to be used with a generic language dialect and using the Go
generics feature. This made it especially complicated to implement type
assertions and type conversion for the generated go code, this is the reason
for the introduction of the functions presented in
\autoref{sec:type-system-clashes} and the need for including the implementation
of these functions when defining a language dialect for the runtime.

\section{Differing Approaches \& Future Work}

While a just in time compiler can substantially improve the performance of a
runtime for long running applications and hot paths, there are a lot of
different approaches to improve a runtime. These approaches often do not
introduce the large technical debt of having to maintain a code generation
backend for multiple architectures or the need for a code generation step as a
whole, contrary to a just in time compilation pipeline. These possible
performance improvement strategies also often do not take a large toll on the
runtime itself to collect meta data of its execution or start a thread
responsible of compiling functions.

An interpreter that walks the abstract syntax tree generated by the parsing of
the input is commonly denoted as the naive implementation of an interpreter.
Most interpreters implement a bytecode compiler and a virtual machine executing
the compiled bytecode. This reduces the amount of data to keep in memory while
switching the evaluation model from traversal to an iteration of a list of
bytes. This solution is often combined with a JIT compiler generating machine
specific instructions comparable to the byte code operators. Furthermore a byte
code interpreter allows for fast variable storage in virtual registers or in a
stack based storage solution \cite{mcilroy2016v8ignition}, \cite[1.
Introduction]{luaReference} and \cite[Introduction]{pythonDev}. 

